{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwRHd3oCgLnfcofk8Eggcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HUMINTING/callte-detetion-detail/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "ab_jS3lzAemh",
        "outputId": "a318b596-d5bf-4922-afac-5cf64b7e9fb6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9d933fe2f6a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimeit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_timer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_nn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os \n",
        "import random \n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from torch import nn,optim\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import pretrainedmodels\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms as T \n",
        "from config import config\n",
        "from PIL import Image \n",
        "from itertools import chain \n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from .augmentations import get_train_transform,get_test_transform\n",
        "import random \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os \n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "from timeit import default_timer as timer\n",
        "from models.my_nn import *\n",
        "from utils import *\n",
        "from IPython import embed\n",
        "#1. set random.seed and cudnn performance\n",
        "random.seed(config.seed)\n",
        "np.random.seed(config.seed)\n",
        "torch.manual_seed(config.seed)\n",
        "torch.cuda.manual_seed_all(config.seed)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpus\n",
        "torch.backends.cudnn.benchmark = True\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class DefaultConfigs(object):\n",
        "  a =  [1, 2, 3, 4, 5 ]\n",
        "  for i in a : \n",
        "    #1.string parameters\n",
        "    train_data = \"/content/drive/MyDrive/estrus_20191230/i/train/\"\n",
        "    test_data = \"content/drive/MyDrive/estrus_20191230/i/test/\"\n",
        "    val_data = \"content/drive/MyDrive/estrus_20191230/i/valid/\"\n",
        "    model_name = \"resnet50\"\n",
        "    weights = \"./checkpoints/\"\n",
        "    best_models = weights + \"best_model/\"\n",
        "    submit = \"./submit/\"\n",
        "    logs = \"./logs/\"\n",
        "    gpus = \"1\"\n",
        "\n",
        "    #2.numeric parameters\n",
        "    epochs = 30\n",
        "    batch_size = 4\n",
        "    img_height = 224\n",
        "    img_weight = 224\n",
        "    num_classes = 62\n",
        "    seed = 888\n",
        "    lr = 1e-3\n",
        "    lr_decay = 1e-4\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "config = DefaultConfigs()\n",
        "\n",
        "class EstrusDataset(Dataset):\n",
        "    def __init__(self,label_list,train=True,test=False):\n",
        "        self.test = test \n",
        "        self.train = train \n",
        "        imgs = []\n",
        "        if self.test:\n",
        "            for index,row in label_list.iterrows():\n",
        "                imgs.append((row[\"filename\"]))\n",
        "            self.imgs = imgs \n",
        "        else:\n",
        "            for index,row in label_list.iterrows():\n",
        "                imgs.append((row[\"filename\"],row[\"label\"]))\n",
        "            self.imgs = imgs\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        if self.test:\n",
        "            filename = self.imgs[index]\n",
        "            img = cv2.imread(filename)\n",
        "            img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img,(int(config.img_height*1.5),int(config.img_weight*1.5)))\n",
        "            img = get_test_transform(img.shape)(image=img)[\"image\"]\n",
        "            return img,filename\n",
        "        else:\n",
        "            filename,label = self.imgs[index] \n",
        "            img = cv2.imread(filename)\n",
        "            img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img,(int(config.img_height*1.5),int(config.img_weight*1.5)))\n",
        "            img = get_train_transform(img.shape,augmentation=config.augmen_level)(image=img)[\"image\"]\n",
        "            return img,label\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "class MyNetBase(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super(MyNetBase, self).__init__()\n",
        "        self.name = name\n",
        "        if name == \"vgg16\":\n",
        "            self.model = models.vgg16(pretrained=True)\n",
        "            layers = list(self.model.classifier.children())[:-2]\n",
        "            self.model.classifier = nn.Sequential(*layers)\n",
        "            self.fc1 = nn.Linear(4096, 1024)\n",
        "            self.drop1 = nn.Dropout(0.5)\n",
        "            self.fc2 = nn.Linear(1024, 2)\n",
        "            \n",
        "        elif name == \"resnet\":\n",
        "            self.model = models.resnet50(pretrained=True)\n",
        "            fc_layer = nn.Sequential(\n",
        "                nn.Linear(2048, 512),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(512, 2)\n",
        "            )\n",
        "            self.model.fc = fc_layer\n",
        "            \n",
        "        elif name == \"densenet\":\n",
        "            self.model = models.densenet161(pretrained=True)\n",
        "            fc_layer = nn.Sequential(\n",
        "                nn.Linear(2208, 512),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(512, 2)\n",
        "            )\n",
        "            self.model.classifier = fc_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.name == \"vgg16\":\n",
        "            h = self.model(x)\n",
        "            h = F.relu(self.drop1(self.fc1(h)))\n",
        "            h = self.fc2(h)\n",
        "        elif self.name == \"resnet\":\n",
        "            h = self.model(x)\n",
        "        elif self.name == \"densenet\":\n",
        "            h = self.model(x)\n",
        "        return h\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    label = []\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0])\n",
        "        label.append(sample[1])\n",
        "\n",
        "    return torch.stack(imgs, 0), \\\n",
        "           label\n",
        "\n",
        "def get_files(root,mode):\n",
        "    #for test\n",
        "    if mode == \"test\":\n",
        "        files = []\n",
        "        for img in os.listdir(root):\n",
        "            files.append(root + img)\n",
        "        files = pd.DataFrame({\"filename\":files})\n",
        "        return files\n",
        "    elif mode != \"test\": \n",
        "        #for train and val       \n",
        "        all_data_path,labels = [],[]\n",
        "        image_folders = list(map(lambda x:root+x,os.listdir(root)))\n",
        "        all_images = list(chain.from_iterable(list(map(lambda x:glob(x+\"/*\"),image_folders))))\n",
        "        print(\"loading train dataset\")\n",
        "        for file in tqdm(all_images):\n",
        "            all_data_path.append(file)\n",
        "            labels.append(int(file.split(\"/\")[-2]))\n",
        "        all_files = pd.DataFrame({\"filename\":all_data_path,\"label\":labels})\n",
        "        return all_files\n",
        "    else:\n",
        "        print(\"check the mode please!\")\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(y_pred, y_actual, topk=(1, )):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = y_actual.size(0)\n",
        "\n",
        "    _, pred = y_pred.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(y_actual.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res\n",
        "\n",
        "#2. evaluate func\n",
        "def evaluate(val_loader,model,criterion,epoch):\n",
        "    #2.1 define meters\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    #progress bar\n",
        "    val_progressor = ProgressBar(mode=\"Val  \",epoch=epoch,total_epoch=config.epochs,model_name=config.model_name,total=len(val_loader))\n",
        "    #2.2 switch to evaluate mode and confirm model has been transfered to cuda\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i,(input,target) in enumerate(val_loader):\n",
        "            val_progressor.current = i \n",
        "            input = Variable(input).cuda()\n",
        "            target = Variable(torch.from_numpy(np.array(target)).long()).cuda()\n",
        "            #target = Variable(target).cuda()\n",
        "            #2.2.1 compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output,target)\n",
        "\n",
        "            #2.2.2 measure accuracy and record loss\n",
        "            precision1,precision2 = accuracy(output,target,topk=(1,2))\n",
        "            losses.update(loss.item(),input.size(0))\n",
        "            top1.update(precision1[0],input.size(0))\n",
        "            val_progressor.current_loss = losses.avg\n",
        "            val_progressor.current_top1 = top1.avg\n",
        "            val_progressor()\n",
        "        val_progressor.done()\n",
        "    return [losses.avg,top1.avg]\n",
        "\n",
        "#3. test model on public dataset and save the probability matrix\n",
        "def test(test_loader,model,folds):\n",
        "    #3.1 confirm the model converted to cuda\n",
        "    csv_map = OrderedDict({\"filename\":[],\"probability\":[]})\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    for i,(input,filepath) in enumerate(tqdm(test_loader)):\n",
        "        #3.2 change everything to cuda and get only basename\n",
        "        filepath = [os.path.basename(x) for x in filepath]\n",
        "        with torch.no_grad():\n",
        "            image_var = Variable(input).cuda()\n",
        "            #3.3.output\n",
        "            #print(filepath)\n",
        "            #print(input,input.shape)\n",
        "            y_pred = model(image_var)\n",
        "            print(y_pred.shape)\n",
        "            smax = nn.Softmax(1)\n",
        "            smax_out = smax(y_pred)\n",
        "        #3.4 save probability to csv files\n",
        "        csv_map[\"filename\"].extend(filepath)\n",
        "        for output in smax_out:\n",
        "            prob = \";\".join([str(i) for i in output.data.tolist()])\n",
        "            csv_map[\"probability\"].append(prob)\n",
        "    result = pd.DataFrame(csv_map)\n",
        "    result[\"probability\"] = result[\"probability\"].map(lambda x : [float(i) for i in x.split(\";\")])\n",
        "    result.to_csv(\"./submit/{}_submission.csv\" .format(config.model_name + \"_\" + str(folds)),index=False,header = None)\n",
        "\n",
        "#4. more details to build main function    \n",
        "def main():\n",
        "    fold = 0\n",
        "    #4.1 mkdirs\n",
        "    if not os.path.exists(config.submit):\n",
        "        os.mkdir(config.submit)\n",
        "    if not os.path.exists(config.weights):\n",
        "        os.mkdir(config.weights)\n",
        "    if not os.path.exists(config.best_models):\n",
        "        os.mkdir(config.best_models)\n",
        "    if not os.path.exists(config.logs):\n",
        "        os.mkdir(config.logs)\n",
        "    if not os.path.exists(config.weights + config.model_name + os.sep +str(fold) + os.sep):\n",
        "        os.makedirs(config.weights + config.model_name + os.sep +str(fold) + os.sep)\n",
        "    if not os.path.exists(config.best_models + config.model_name + os.sep +str(fold) + os.sep):\n",
        "        os.makedirs(config.best_models + config.model_name + os.sep +str(fold) + os.sep)       \n",
        "    #4.2 get model and optimizer\n",
        "    model = MyNetBase()\n",
        "    #model = torch.nn.DataParallel(model)\n",
        "    model.cuda()\n",
        "\n",
        "    #optimizer = optim.SGD(model.parameters(),lr = config.lr,momentum=0.9,weight_decay=config.weight_decay)\n",
        "    optimizer = optim.Adam(model.parameters(),lr = config.lr,amsgrad=True,weight_decay=config.weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    #4.3 some parameters for  K-fold and restart model\n",
        "    start_epoch = 0\n",
        "    best_precision1 = 0\n",
        "    best_precision_save = 0\n",
        "    resume = False\n",
        "    \n",
        "    #4.4 restart the training process\n",
        "    if resume:\n",
        "        checkpoint = torch.load(config.best_models + str(fold) + \"/model_best.pth.tar\")\n",
        "        start_epoch = checkpoint[\"epoch\"]\n",
        "        fold = checkpoint[\"fold\"]\n",
        "        best_precision1 = checkpoint[\"best_precision1\"]\n",
        "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    #4.5 get files and split for K-fold dataset\n",
        "    #4.5.1 read files\n",
        "    train_data_list = get_files(config.train_data,\"train\")\n",
        "    val_data_list = get_files(config.val_data,\"val\")\n",
        "    #test_files = get_files(config.test_data,\"test\")\n",
        "\n",
        "    \"\"\" \n",
        "    #4.5.2 split\n",
        "    split_fold = StratifiedKFold(n_splits=3)\n",
        "    folds_indexes = split_fold.split(X=origin_files[\"filename\"],y=origin_files[\"label\"])\n",
        "    folds_indexes = np.array(list(folds_indexes))\n",
        "    fold_index = folds_indexes[fold]\n",
        "\n",
        "    #4.5.3 using fold index to split for train data and val data\n",
        "    train_data_list = pd.concat([origin_files[\"filename\"][fold_index[0]],origin_files[\"label\"][fold_index[0]]],axis=1)\n",
        "    val_data_list = pd.concat([origin_files[\"filename\"][fold_index[1]],origin_files[\"label\"][fold_index[1]]],axis=1)\n",
        "    \"\"\"\n",
        "    #train_data_list,val_data_list = train_test_split(origin_files,test_size = 0.1,stratify=origin_files[\"label\"])\n",
        "    #4.5.4 load dataset\n",
        "    train_dataloader = DataLoader(EstrusDataset(train_data_list),batch_size=config.batch_size,shuffle=True,collate_fn=collate_fn,pin_memory=True,num_workers=4)\n",
        "    val_dataloader = DataLoader(EstrusDataset(val_data_list,train=False),batch_size=config.batch_size * 2,shuffle=True,collate_fn=collate_fn,pin_memory=False,num_workers=4)\n",
        "    #test_dataloader = DataLoader(ChaojieDataset(test_files,test=True),batch_size=1,shuffle=False,pin_memory=False)\n",
        "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\"max\",verbose=1,patience=3)\n",
        "    scheduler =  optim.lr_scheduler.StepLR(optimizer,step_size = 10,gamma=0.1)\n",
        "    #4.5.5.1 define metrics\n",
        "    train_losses = AverageMeter()\n",
        "    train_top1 = AverageMeter()\n",
        "    valid_loss = [np.inf,0,0]\n",
        "    model.train()\n",
        "\n",
        "    #4.5.5 train\n",
        "    start = timer()\n",
        "    for epoch in range(start_epoch,config.epochs):\n",
        "        scheduler.step(epoch)\n",
        "        train_progressor = ProgressBar(mode=\"Train\",epoch=epoch,total_epoch=config.epochs,model_name=config.model_name,total=len(train_dataloader))\n",
        "        # train\n",
        "        #global iter\n",
        "        for iter,(input,target) in enumerate(train_dataloader):\n",
        "            #4.5.5 switch to continue train process\n",
        "            train_progressor.current = iter\n",
        "            model.train()\n",
        "            input = Variable(input).cuda()\n",
        "            target = Variable(torch.from_numpy(np.array(target)).long()).cuda()\n",
        "            #target = Variable(target).cuda()\n",
        "            output = model(input)\n",
        "            loss = criterion(output,target)\n",
        "\n",
        "            precision1_train,precision2_train = accuracy(output,target,topk=(1,2))\n",
        "            train_losses.update(loss.item(),input.size(0))\n",
        "            train_top1.update(precision1_train[0],input.size(0))\n",
        "            train_progressor.current_loss = train_losses.avg\n",
        "            train_progressor.current_top1 = train_top1.avg\n",
        "            #backward\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_progressor()\n",
        "        train_progressor.done()\n",
        "        #evaluate\n",
        "        lr = get_learning_rate(optimizer)\n",
        "        #evaluate every half epoch\n",
        "        valid_loss = evaluate(val_dataloader,model,criterion,epoch)\n",
        "        is_best = valid_loss[1] > best_precision1\n",
        "        best_precision1 = max(valid_loss[1],best_precision1)\n",
        "        try:\n",
        "            best_precision_save = best_precision1.cpu().data.numpy()\n",
        "        except:\n",
        "            pass\n",
        "        save_checkpoint({\n",
        "                    \"epoch\":epoch + 1,\n",
        "                    \"model_name\":config.model_name,\n",
        "                    \"state_dict\":model.state_dict(),\n",
        "                    \"best_precision1\":best_precision1,\n",
        "                    \"optimizer\":optimizer.state_dict(),\n",
        "                    \"fold\":fold,\n",
        "                    \"valid_loss\":valid_loss,\n",
        "        },is_best,fold)\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PNTs6HsCCvg",
        "outputId": "d6f02231-16da-4f91-b08c-f1f146d2a925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:File `'main.py'` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg4yoOUmBQAo",
        "outputId": "ef48871b-df3f-41fd-bb91-3d4e10d74f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}