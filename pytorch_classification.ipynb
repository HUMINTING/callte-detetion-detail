{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ElzRDu37J4873B2fHYRFnaz6g-P5N2Hg",
      "authorship_tag": "ABX9TyMO+aQhg3YI6rjbesmLlevh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HUMINTING/callte-detetion-detail/blob/main/pytorch_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCaO9vmO75Dm",
        "outputId": "a60aab3f-b51e-431e-e069-b3d62646da90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pretrainedmodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yaea3ap_t0O",
        "outputId": "1d9123b3-763d-4caf-eaea-a21acb13d956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pretrainedmodels\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (0.13.1+cu113)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pretrainedmodels) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pretrainedmodels) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pretrainedmodels) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pretrainedmodels) (3.0.4)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60966 sha256=af131c42d157c806e12843bafba315dba021f44c5f372afa65284e2ee5a707b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "%matplotlib inline\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import time\n",
        "import datetime as dt\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import pretrainedmodels\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pickle\n",
        "import math\n",
        "import glob\n",
        "from sklearn import random_projection\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i87YNHomCIZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super(Net, self).__init__()\n",
        "        self.name = name\n",
        "        if name == \"vgg16\":\n",
        "            self.model = models.vgg16(pretrained=True)\n",
        "            layers = list(self.model.classifier.children())[:-2]\n",
        "            self.model.classifier = nn.Sequential(*layers)\n",
        "            self.fc1 = nn.Linear(4096, 1024)\n",
        "            self.drop1 = nn.Dropout(0.5)\n",
        "            self.fc2 = nn.Linear(1024, 2)\n",
        "            \n",
        "        elif name == \"resnet\":\n",
        "            self.model = models.resnet50(pretrained=True)\n",
        "            fc_layer = nn.Sequential(\n",
        "                nn.Linear(2048, 512),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(512, 2)\n",
        "            )\n",
        "            self.model.fc = fc_layer\n",
        "            \n",
        "        elif name == \"densenet\":\n",
        "            self.model = models.densenet161(pretrained=True)\n",
        "            fc_layer = nn.Sequential(\n",
        "                nn.Linear(2208, 512),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(512, 2)\n",
        "            )\n",
        "            self.model.classifier = fc_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.name == \"vgg16\":\n",
        "            h = self.model(x)\n",
        "            h = F.relu(self.drop1(self.fc1(h)))\n",
        "            h = self.fc2(h)\n",
        "        elif self.name == \"resnet\":\n",
        "            h = self.model(x)\n",
        "        elif self.name == \"densenet\":\n",
        "            h = self.model(x)\n",
        "        return h"
      ],
      "metadata": {
        "id": "ZaudFiNzCMlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "home = os.path.expanduser('~')\n",
        "\n",
        "##数据集的类别\n",
        "NUM_CLASSES = 206\n",
        "\n",
        "#训练时batch的大小\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "#网络默认输入图像的大小\n",
        "INPUT_SIZE = 300\n",
        "#训练最多的epoch\n",
        "MAX_EPOCH = 100\n",
        "# 使用gpu的数目\n",
        "GPUS = 2\n",
        "# 从第几个epoch开始resume训练，如果为0，从头开始\n",
        "RESUME_EPOCH = 0\n",
        "\n",
        "WEIGHT_DECAY = 5e-4\n",
        "MOMENTUM = 0.9\n",
        "# 初始学习率\n",
        "LR = 1e-3\n",
        "\n",
        "BASE =  '/content/drive/MyDrive/estrus_20191230/1/'\n",
        "\n",
        "# 训练好模型的保存位置\n",
        "SAVE_FOLDER = '/content/drive/MyDrive/estrus_20191230/save_weight/'\n",
        "#数据集的存放位置\n",
        "TRAIN_LABEL_DIR =BASE + 'train.txt'     \n",
        "VAL_LABEL_DIR = BASE + 'val.txt'\n",
        "TEST_LABEL_DIR = BASE + 'test.txt'\n",
        "#训练完成，权重文件的保存路径,默认保存在trained_model下\n",
        "TRAINED_MODEL = BASE + 'weights/resnet/epoch_40.pth'\n",
        "\n",
        "# 采用的模型名称\n",
        "model_name = 'resnext'"
      ],
      "metadata": {
        "id": "tcYEicu1Ijy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transform\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "from torchvision import transforms\n",
        "\n",
        "class Resize(object):\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # padding\n",
        "        ratio = self.size[0] / self.size[1]\n",
        "        w, h = img.size\n",
        "        if w / h < ratio:\n",
        "            t = int(h * ratio)\n",
        "            w_padding = (t - w) // 2\n",
        "            img = img.crop((-w_padding, 0, w+w_padding, h))\n",
        "        else:\n",
        "            t = int(w / ratio)\n",
        "            h_padding = (t - h) // 2\n",
        "            img = img.crop((0, -h_padding, w, h+h_padding))\n",
        "\n",
        "        img = img.resize(self.size, self.interpolation)\n",
        "\n",
        "        return img\n",
        "\n",
        "class RandomRotate(object):\n",
        "    def __init__(self, degree, p=0.5):\n",
        "        self.degree = degree\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p:\n",
        "            rotate_degree = random.uniform(-1*self.degree, self.degree)\n",
        "            img = img.rotate(rotate_degree, Image.BILINEAR)\n",
        "        return img\n",
        "\n",
        "class RandomGaussianBlur(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p:\n",
        "            img = img.filter(ImageFilter.GaussianBlur(\n",
        "                radius=random.random()))\n",
        "        return img\n",
        "\n",
        "\n",
        "\n",
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "def get_train_transform(mean=mean, std=std, size=0):\n",
        "    train_transform = transforms.Compose([\n",
        "        Resize((int(size * (256 / 224)), int(size * (256 / 224)))),\n",
        "        transforms.RandomCrop(size),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        RandomRotate(15, 0.3),\n",
        "        # RandomGaussianBlur(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "    return train_transform\n",
        "\n",
        "def get_test_transform(mean=mean, std=std, size=0):\n",
        "    return transforms.Compose([\n",
        "        Resize((int(size * (256 / 224)), int(size * (256 / 224)))),\n",
        "        transforms.CenterCrop(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "\n",
        "def tta_test_transform(mean=mean, std=std, size=0):\n",
        "    return transforms.Compose([\n",
        "        Resize((int(size * (256 / 224)), int(size * (256 / 224)))),\n",
        "        # transforms.CenterCrop(size),\n",
        "        transforms.RandomCrop(size),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        RandomRotate(15, 0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ])"
      ],
      "metadata": {
        "id": "bo_52PN_F4GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import sys \n",
        "sys.path.append(\"..\") \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_size = INPUT_SIZE\n",
        "batch_size = BATCH_SIZE\n",
        "\n",
        "# 构建数据提取器，利用dataloader\n",
        "# 利用torchvision中的transforms进行图像预处理\n",
        "#cfg为config文件，保存几个方便修改的参数\n",
        "\n",
        "class  SelfCustomDataset(Dataset):\n",
        "    def __init__(self, label_file, imageset):\n",
        "        '''\n",
        "        img_dir: 图片路径：img_dir + img_name.jpg构成图片的完整路径      \n",
        "        '''\n",
        "        # 所有图片的绝对路径\n",
        "        with open(label_file, 'r') as f:\n",
        "            #label_file的格式， （label_file image_label)\n",
        "            self.imgs = list(map(lambda line: line.strip().split(' '), f))\n",
        "      # 相关预处理的初始化\n",
        "      #   self.transforms=transform\n",
        "        self.img_aug=True\n",
        "        if imageset == 'train':\n",
        "            self.transform= get_train_transform(size=INPUT_SIZE)\n",
        "        else:\n",
        "            self.transform = get_test_transform(size = INPUT_SIZE)\n",
        "        self.input_size = INPUT_SIZE\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path, label = self.imgs[index]\n",
        "        # print(img_path)\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.img_aug:\n",
        "            img =self.transform(img)\n",
        "\n",
        "\n",
        "        else:\n",
        "            img = np.array(img)\n",
        "            img = torch.from_numpy(img)\n",
        "\n",
        "        return img, torch.from_numpy(np.array(int(label)))\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n",
        "\n",
        "##ImageFolder对象可以将一个文件夹下的文件构造成一类\n",
        "#所以数据集的存储格式为一个类的图片放置到一个文件夹下\n",
        "#然后利用dataloader构建提取器，每次返回一个batch的数据，在很多情况下，利用num_worker参数\n",
        "#设置多线程，来相对提升数据提取的速度\n",
        "\n",
        "train_label_dir = TRAIN_LABEL_DIR\n",
        "train_datasets = SelfCustomDataset(train_label_dir, imageset='train')\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "val_label_dir = VAL_LABEL_DIR\n",
        "val_datasets = SelfCustomDataset(val_label_dir, imageset='test')\n",
        "val_dataloader = torch.utils.data.DataLoader(val_datasets, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "##进行数据提取函数的测试\n",
        "if __name__ ==\"__main__\":\n",
        "\n",
        "    for images, labels in train_dataloader:\n",
        "        print(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "P-PoJo4yMBZ2",
        "outputId": "fe7ac453-0a43-4813-f1a2-1cad8619f643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8e201ed7ecd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mtrain_label_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAIN_LABEL_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtrain_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelfCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8e201ed7ecd3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, label_file, imageset)\u001b[0m\n\u001b[1;32m     26\u001b[0m         '''\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# 所有图片的绝对路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m#label_file的格式， （label_file image_label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/estrus_20191230/1/train.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*-coding:utf-8-*-\n",
        "\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "\n",
        "def write_txt(content, filename, mode='w'):\n",
        "    \"\"\"保存txt数据\n",
        "    :param content:需要保存的数据,type->list\n",
        "    :param filename:文件名\n",
        "    \"\"\"\n",
        "    with open(filename, mode) as f:\n",
        "        for line in content:\n",
        "            str_line = \"\"\n",
        "            for col, data in enumerate(line):\n",
        "                if not col == len(line) - 1:\n",
        "                    # 以空格作为分隔符\n",
        "                    str_line = str_line + str(data) + \" \"\n",
        "                else:\n",
        "                    # 每行最后一个数据用换行符“\\n”\n",
        "                    str_line = str_line + str(data) + \"\\n\"\n",
        "            f.write(str_line)\n",
        "\n",
        "\n",
        "def get_files_list(dir):\n",
        "    '''\n",
        "    实现遍历dir目录下,所有文件(包含子文件夹的文件)\n",
        "    :param dir:指定文件夹目录\n",
        "    :return:包含所有文件的列表->list\n",
        "    '''\n",
        "    # parent:父目录, filenames:该目录下所有文件夹,filenames:该目录下的文件名\n",
        "    files_list = [] #写入文件的数据\n",
        "    for parent, dirnames, filenames in os.walk(dir):\n",
        "        for filename in filenames:\n",
        "            print(\"parent is: \" + parent)\n",
        "            print(\"filename is: \" + filename)\n",
        "            print(os.path.join(parent, filename).replace('\\\\','/'))  # 输出rootdir路径下所有文件（包含子文件）信息\n",
        "            curr_file = parent.split(os.sep)[-1]\t#获取正在遍历的文件夹名（也就是类名）\n",
        "\t\t\t#根据class名确定labels\n",
        "            if curr_file == \"mt\":\n",
        "                labels = 0\n",
        "            elif curr_file == \"one\":\n",
        "                labels = 1\n",
        "            elif curr_file == \"multiple\":\n",
        "                labels = 2\n",
        "            elif curr_file == \"garbage\":\n",
        "                labels = 3\n",
        "\n",
        "       \t\t  dir_path = parent.replace('\\\\','/').split('/')[-2]  #train?val?test?\n",
        "\n",
        "            curr_file = os.path.join(dir_path, curr_file)  #相对路径\n",
        "\n",
        "            files_list.append([os.path.join(curr_file, filename).replace('\\\\','/'), labels])\t#相对路径+label\n",
        "\n",
        "\n",
        "    \t\t#写入csv文件\n",
        "            path = \"%s\" % os.path.join(curr_file, filename).replace('\\\\','/')\n",
        "            label = \"%d\" % labels\n",
        "            list = [path, label]\n",
        "            data = pd.DataFrame([list])\n",
        "            if dir == '/content/drive/MyDrive/estrus_20191230/1/train':\n",
        "                data.to_csv(\"./content/drive/MyDrive/estrus_20191230/1/train.csv\", mode='a', header=False, index=False)\n",
        "            elif dir == '/content/drive/MyDrive/estrus_20191230/1/valid':\n",
        "                data.to_csv(\"/content/drive/MyDrive/estrus_20191230/1/val.csv\", mode='a', header=False, index=False)\n",
        "            elif dir == '/content/drive/MyDrive/estrus_20191230/1/test':\n",
        "                data.to_csv(\"/content/drive/MyDrive/estrus_20191230/1/test.csv\", mode='a', header=False, index=False)\n",
        "    \n",
        "    return files_list\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    import pandas as pd\n",
        "    #先生成两个csv文件夹\n",
        "    df = pd.DataFrame(columns=['path', 'label'])\n",
        "    df.to_csv(\"/content/drive/MyDrive/estrus_20191230/1/train.csv\", index=False)\n",
        "\n",
        "    df2 = pd.DataFrame(columns=['path', 'label'])\n",
        "    df2.to_csv(\"/content/drive/MyDrive/estrus_20191230/1/val.csv\", index=False)\n",
        "\n",
        "    df3 = pd.DataFrame(columns=['path', 'label'])\n",
        "    df3.to_csv(\"/content/drive/MyDrive/estrus_20191230/1/test.csv\", index=False)\n",
        "\n",
        "    #写入txt文件\n",
        "    train_dir = '/content/drive/MyDrive/estrus_20191230/1/train'\n",
        "    train_txt = '/content/drive/MyDrive/estrus_20191230/1/train.txt'\n",
        "    train_data = get_files_list(train_dir)\n",
        "    write_txt(train_data, train_txt, mode='w')\n",
        "\n",
        "    val_dir = '/content/drive/MyDrive/estrus_20191230/1/valid'\n",
        "    val_txt = '/content/drive/MyDrive/estrus_20191230/1/val.txt'\n",
        "    val_data = get_files_list(val_dir)\n",
        "    write_txt(val_data, val_txt, mode='w')\n",
        "\n",
        "    train_dir = '/content/drive/MyDrive/estrus_20191230/1/test'\n",
        "    train_txt = '/content/drive/MyDrive/estrus_20191230/1/test.txt'\n",
        "    train_data = get_files_list(train_dir)\n",
        "    write_txt(train_data, train_txt, mode='w')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "mwp8N56va2dc",
        "outputId": "53065401-3271-4158-8b1b-05522d24042d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-c38a943e81f0>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    dir_path = parent.replace('\\\\','/').split()[-2]  #train?val?test?\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    }
  ]
}